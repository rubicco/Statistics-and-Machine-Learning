{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3: Text clustering and topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contributers:**  \n",
    "Mim Kemal Tekin (mimte666)  \n",
    "Andreas Stasinakis (andst475)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text clustering groups documents in such a way that documents within a group are more &lsquo;similar&rsquo; to other documents in the cluster than to documents not in the cluster. The exact definition of what &lsquo;similar&rsquo; means in this context varies across applications and clustering algorithms.\n",
    "\n",
    "In this lab you will experiment with both hard and soft clustering techniques. More specifically, in the first part you will be using the $k$-means algorithm, and in the second part you will be using a topic model based on the Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard clustering data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data for the hard clustering part of this lab is a collection of product reviews. We have preprocessed the data by tokenization and lowercasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "\n",
    "with bz2.open(\"reviews.json.bz2\") as source:\n",
    "    df = pd.read_json(source)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>music</td>\n",
       "      <td>neg</td>\n",
       "      <td>i bought this album because i loved the title ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>music</td>\n",
       "      <td>neg</td>\n",
       "      <td>i was misled and thought i was buying the enti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>books</td>\n",
       "      <td>neg</td>\n",
       "      <td>i have introduced many of my ell , high school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>books</td>\n",
       "      <td>pos</td>\n",
       "      <td>anything you purchase in the left behind serie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dvd</td>\n",
       "      <td>pos</td>\n",
       "      <td>i loved these movies , and i cant wiat for the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category sentiment                                               text\n",
       "0    music       neg  i bought this album because i loved the title ...\n",
       "1    music       neg  i was misled and thought i was buying the enti...\n",
       "2    books       neg  i have introduced many of my ell , high school...\n",
       "3    books       pos  anything you purchase in the left behind serie...\n",
       "4      dvd       pos  i loved these movies , and i cant wiat for the..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['music', 'books', 'dvd', 'camera', 'health', 'software'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "books       2000\n",
       "health      2000\n",
       "music       2000\n",
       "dvd         2000\n",
       "camera      1999\n",
       "software    1915\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you inspect the data frame, you can see that there are three labelled columns: `category` (the product category), `sentiment` (whether the product review was classified as &lsquo;positive&rsquo; or &lsquo;negative&rsquo; towards the product), and `text` (the space-separated text of the review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Problem 1: K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to cluster the product review data using a tfâ€“idf vectorizer and a $k$-means clusterer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by doing the vectorization. In connection with vectorization, you should also filter out standard English stop words. While you could use [spaCy](https://spacy.io/) for this task, here it suffices to use the word list implemented in [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING - VECTORIZATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here the data has already preprocessed for lower case and tokenazation.\n",
    "#Therefore we just need to use stop words in the vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "reviews = vectorizer.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your vectorization by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11914, 46619)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used the English stop word list from scikit-learn, then the resulting vocabulary should have 46,619 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, cluster the vectorized data. Before doing so, you should read the documentation of the [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) class, which is scikit-learn&rsquo;s implementation of the $k$-means algorithm. As you can see, this class has several parameters that you can tweak. For now, the only parameter that you will have to set is the number of clusters. We recommend that you choose $k=3$.\n",
    "\n",
    "**Tip:** Training $k$-means models will take some time. To speed things up, you can use the `n_init` parameter to control the number of times that the clustering is re-computed with different initial values. The default value for this parameter is 10; here and in the rest of this lab, you may want to set this to a lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=3, n_init=5, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=12345, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the K means classifier\n",
    "Nclus = 3 #number of clusters MUST pre defined for K means algo\n",
    "n_init = 5 #number of restarting the algorithm\n",
    "kmeans_3 = KMeans(n_clusters=Nclus,\n",
    "                n_init= n_init,\n",
    "                random_state = 12345)\n",
    "\n",
    "#fit the classifier with the data\n",
    "kmeans_3.fit(reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=6, n_init=5, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=12345, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now use it for 6 clusters\n",
    "#Create the K means classifier\n",
    "Nclus = 6\n",
    "n_init = 5\n",
    "kmeans_6 = KMeans(n_clusters=Nclus\n",
    "                  ,n_init= n_init, \n",
    "                  random_state = 12345)\n",
    "\n",
    "#fit the classifier with the data\n",
    "kmeans_6.fit(reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sanity-check your clustering, create a bar plot with the number of documents per cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting the 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.unique(kmeans_3.labels_,return_counts=True)[0]\n",
    "y = np.unique(kmeans_3.labels_,return_counts=True)[1]\n",
    "\n",
    "\n",
    "plt.bar(x, y, color='blue', edgecolor='white',width = 0.5)\n",
    "\n",
    "plt.xlabel('Clusters', fontweight='bold')\n",
    "\n",
    "plt.title(\"Text in each Cluster\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sizes may vary considerable between clusters and among different random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Summarize clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a clustering, you can try to see whether it is meaningful. One useful technique in that context is to generate a **summary** for each cluster by extracting the $n$ highest-weighted terms from the centroid of each cluster. Your next task is to implement this approach.\n",
    "\n",
    "**Hint:** You will need to construct an &lsquo;inverted vocabulary&rsquo; that allows you to map from the index of a term back to the original term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this chunk we take the first N indexes of each cluster with highest weight term\n",
    "#Now we will create a summary for each of the Clusters\n",
    "#We will extract the most common words from each cluster in order to have an idea for each cluster\n",
    "\n",
    "n = 15\n",
    "Nclus = 3  \n",
    "ind = np.zeros((Nclus,n))\n",
    "\n",
    "for i in  range( kmeans_3.cluster_centers_.shape[0]):\n",
    "    \n",
    "    ind[i] = kmeans_3.cluster_centers_[i].argsort()[::-1][:n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like</td>\n",
       "      <td>product</td>\n",
       "      <td>movie</td>\n",
       "      <td>great</td>\n",
       "      <td>just</td>\n",
       "      <td>good</td>\n",
       "      <td>cd</td>\n",
       "      <td>album</td>\n",
       "      <td>use</td>\n",
       "      <td>time</td>\n",
       "      <td>does</td>\n",
       "      <td>did</td>\n",
       "      <td>really</td>\n",
       "      <td>music</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>read</td>\n",
       "      <td>books</td>\n",
       "      <td>author</td>\n",
       "      <td>reading</td>\n",
       "      <td>story</td>\n",
       "      <td>quot</td>\n",
       "      <td>like</td>\n",
       "      <td>just</td>\n",
       "      <td>good</td>\n",
       "      <td>written</td>\n",
       "      <td>did</td>\n",
       "      <td>great</td>\n",
       "      <td>really</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>camera</td>\n",
       "      <td>lens</td>\n",
       "      <td>pictures</td>\n",
       "      <td>canon</td>\n",
       "      <td>digital</td>\n",
       "      <td>use</td>\n",
       "      <td>battery</td>\n",
       "      <td>flash</td>\n",
       "      <td>quality</td>\n",
       "      <td>great</td>\n",
       "      <td>case</td>\n",
       "      <td>good</td>\n",
       "      <td>cameras</td>\n",
       "      <td>picture</td>\n",
       "      <td>zoom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1         2       3        4      5        6      7        8  \\\n",
       "0    like  product     movie   great     just   good       cd  album      use   \n",
       "1    book     read     books  author  reading  story     quot   like     just   \n",
       "2  camera     lens  pictures   canon  digital    use  battery  flash  quality   \n",
       "\n",
       "       9       10    11       12       13      14  \n",
       "0   time     does   did   really    music    work  \n",
       "1   good  written   did    great   really  people  \n",
       "2  great     case  good  cameras  picture    zoom  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List comprehantion for iterating each index\n",
    "summary = [[vectorizer.get_feature_names()[int(k)] for k in i] for i in ind]\n",
    "pd.DataFrame(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movie</td>\n",
       "      <td>film</td>\n",
       "      <td>movies</td>\n",
       "      <td>like</td>\n",
       "      <td>story</td>\n",
       "      <td>watch</td>\n",
       "      <td>just</td>\n",
       "      <td>good</td>\n",
       "      <td>great</td>\n",
       "      <td>acting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>read</td>\n",
       "      <td>books</td>\n",
       "      <td>author</td>\n",
       "      <td>reading</td>\n",
       "      <td>story</td>\n",
       "      <td>like</td>\n",
       "      <td>quot</td>\n",
       "      <td>just</td>\n",
       "      <td>written</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>camera</td>\n",
       "      <td>lens</td>\n",
       "      <td>pictures</td>\n",
       "      <td>canon</td>\n",
       "      <td>use</td>\n",
       "      <td>digital</td>\n",
       "      <td>flash</td>\n",
       "      <td>battery</td>\n",
       "      <td>quality</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>software</td>\n",
       "      <td>program</td>\n",
       "      <td>product</td>\n",
       "      <td>version</td>\n",
       "      <td>use</td>\n",
       "      <td>computer</td>\n",
       "      <td>support</td>\n",
       "      <td>windows</td>\n",
       "      <td>easy</td>\n",
       "      <td>microsoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product</td>\n",
       "      <td>great</td>\n",
       "      <td>good</td>\n",
       "      <td>like</td>\n",
       "      <td>use</td>\n",
       "      <td>just</td>\n",
       "      <td>does</td>\n",
       "      <td>hair</td>\n",
       "      <td>time</td>\n",
       "      <td>did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>album</td>\n",
       "      <td>cd</td>\n",
       "      <td>music</td>\n",
       "      <td>songs</td>\n",
       "      <td>quot</td>\n",
       "      <td>song</td>\n",
       "      <td>like</td>\n",
       "      <td>just</td>\n",
       "      <td>great</td>\n",
       "      <td>band</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1         2        3        4         5        6        7  \\\n",
       "0     movie     film    movies     like    story     watch     just     good   \n",
       "1      book     read     books   author  reading     story     like     quot   \n",
       "2    camera     lens  pictures    canon      use   digital    flash  battery   \n",
       "3  software  program   product  version      use  computer  support  windows   \n",
       "4   product    great      good     like      use      just     does     hair   \n",
       "5     album       cd     music    songs     quot      song     like     just   \n",
       "\n",
       "         8          9  \n",
       "0    great     acting  \n",
       "1     just    written  \n",
       "2  quality      great  \n",
       "3     easy  microsoft  \n",
       "4     time        did  \n",
       "5    great       band  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nclus = 6\n",
    "n = 10\n",
    "  \n",
    "ind = np.zeros((Nclus,n))\n",
    "\n",
    "for i in  range( kmeans_6.cluster_centers_.shape[0]):\n",
    "    \n",
    "    ind[i] = kmeans_6.cluster_centers_[i].argsort()[::-1][:n]\n",
    "\n",
    "#List comprehantion for iterating each index\n",
    "summary_K6 = [[vectorizer.get_feature_names()[int(k)] for k in i] for i in ind]\n",
    "pd.DataFrame(summary_K6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have computed the cluster summaries, discuss their quality. Is it clear what the reviews in a given cluster are about? Which clusters are clearest? Which are less clear? Do the cluster summaries contain any unexpected terms? What happens if you re-cluster with, say, $k=6$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that specific problem, we have the class for each review. In that way, we can see in advance that the number of clusters is 6. Therefore, we are expecting that K = 3 may combine different \"similar\" classes together.  \n",
    "\n",
    "When we use 3 clusters, the model is not able to seperate the category \"Dvd\" and \"Music\". The two other clusters can be easily classified as the \"Book\" and \"Camera\". It is clear that the algorithm can not capture the classes \"Software\" and \"Health\" though.\n",
    "\n",
    "On the other hand, when we use 6 clusters(which is the real number of the clusters), all of the classes are easily recognissed.\n",
    "\n",
    "We could also do the lemmatation in the pre processing procedure in order to avoid repetation of some words like did and does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Tune the k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major limitation of the $k$-means algorithm is that one has to manually set the value for $k$, the number of clusters. One heuristic that can help you with this is the [Elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)). Your next task is to implement this method to see whether it allows you to find a better value for $k$.\n",
    "\n",
    "To follow the elbow method, you should plot different values of $k$ against the **inertia** (sums of squared distances between documents and closest centroids) of the fitted $k$-means model, and pick the $k$ at the &lsquo;elbow point&rsquo; of the resulting graph. Test cluster sizes between 1 and 9.\n",
    "\n",
    "**Note that this will take a while.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Intertias Scores for different K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#We now use the Elbow method\n",
    "#We will run the algorithm many times\n",
    "K = 9\n",
    "Intertias = np.zeros(K)\n",
    "\n",
    "r = 0\n",
    "for i in range(1,K+1):\n",
    "    Intertias[r] = KMeans(n_clusters= i,n_init= 5, random_state = 12345).fit(reviews).inertia_\n",
    "    r = r + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11764.20258591, 11696.56159235, 11652.42954528, 11614.2159932 ,\n",
       "       11578.79453702, 11552.6884815 , 11525.75879495, 11490.98972428,\n",
       "       11478.909646  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Intertias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot number of clusters vs Intertia Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(1,10)]\n",
    "\n",
    "plt.plot(x,Intertias, color = \"red\" ,marker='o', linestyle='dashed',linewidth = 2,markersize = 10,alpha = 0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the diagram, can you see a pronounced &lsquo;elbow point&rsquo;? Discuss your findings in a short text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the true number of classes is 6. Therefore, somenone would expect that the elbow point will be for K = 6. From the plot though, the above statement is not so clear because also K = 8 seems to be good elbow point using the Intertias scores, after that it seems overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Compare clusterings using the Rand index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some scenarios, you may have gold-standard class labels available for at least a subset of your documents. In these cases you can compute the **Rand index** of a clustering, and use this measure to compare the quality of different clusterings.\n",
    "\n",
    "To compute the Rand index, we view a clustering as a binary classifier on pairs of documents. The classifier predicts &lsquo;positive&rsquo; if and only if the two documents belong to the same cluster. The (non-normalized) Rand index of the clustering is the accuracy of this classifier relative to a reference in which a document pair belongs to the &lsquo;positive&rsquo; class if and only if the two documents in the pair have the same gold-standard class label.\n",
    "\n",
    "Compare a clustering with $k=3$ clusters to a second clustering with $k=6$ clusters. As your evaluation data, use the first 500 documents from the original data set along with their gold-standard categories (from the `category` column). What do you observe? How do you interpret your observations? What arguments can you find against the Rand index as a measure for comparing clusterings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rand Index Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#Gold standard classes\n",
    "classes = df[\"category\"][0:501]\n",
    "\n",
    "#The predictions for K = 3\n",
    "clusters_3 = kmeans_3.predict(reviews[0:501])\n",
    "\n",
    "#For K = 6\n",
    "clusters_6 = kmeans_6.predict(reviews[0:501])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a function for the Rand index score\n",
    "\n",
    "#compare the classes and the prediction\n",
    "def compare(x):\n",
    "    res = False\n",
    "    if x[0] ==x[1]:\n",
    "        res = True\n",
    "    return res\n",
    "\n",
    "def RandIndex(preds,y):\n",
    "    \n",
    "    #FIRST STEP\n",
    "    #Our first step is to go through all combination for both the clusters and classes\n",
    "    #For each pair, we see if the two elements belong to the same cluster(or class).\n",
    "    \n",
    "    #first we have to go pairwise through all combinations of the predictions and see how many of them belong to the same cluster\n",
    "    clusters = [compare(i) for i in list(itertools.combinations(preds, 2))] #This gives a new classifier with labels TRUE or FALSE\n",
    "    \n",
    "    #we do the same for the classes\n",
    "    classes =  [compare(i) for i in list(itertools.combinations(y, 2))] #This gives the labels of the classifier TRUE or FALSE\n",
    "     \n",
    "    #Now we have the predictions of the new classifier and the gold classes. We just need the accuracy of that classifier\n",
    "    #We use a nested list comprehantion in order to compute the TP + FN. \n",
    "    TP_FN = sum([compare(pair) for pair in [[clusters[i],classes[i]] for i in (range(0,len(clusters)))]])\n",
    "    \n",
    "    #accuracy\n",
    "    randIndex = TP_FN/len(clusters)\n",
    "    return randIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rand Index for K = 3\n",
    "RandIn_3K = RandIndex(classes,clusters_3)\n",
    "\n",
    "#Rand Index for K = 6\n",
    "RandIn_6K = RandIndex(classes,clusters_6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Rand Index for 3 Clusters is 0.4442874251497006',\n",
       " 'The Rand Index for 6 Clusters is 0.7638483033932135')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The Rand Index for 3 Clusters is \" + str(RandIn_3K), \"The Rand Index for 6 Clusters is \" + str(RandIn_6K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rand Index score measures the similarity between to clustering algorithm and it can be a number between 0 and 1(0 for not related at all, 1 for totaly). In this task we compare the similarity between a clustering and the gold standard classes for two different clustering algorith( K = 3 and K =6). From the scores above, it is clear that Kmeans with 6 clusters performs much better than with 3 clusters.\n",
    "\n",
    "One drawback for RI could be that it takes into accout the FN. That means that, if the data have many different classes, many pairs will have different clusters and true labels in the same time. Therefore, the RI will be high, even if there are not many TP in that score. For those cases, it may be better to use Adjusted Rand Index, an alternative of RI, which does not take into account the False Positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for the topic modelling part of this lab is the collection of all [State of the Union](https://en.wikipedia.org/wiki/State_of_the_Union) addresses from the years 1975â€“2000. These speeches come as a single text file with one sentence per line. The following code cell prints the first 5 lines from the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr speaker mr vice president members of the 94th congress and distinguished guests\n",
      "twenty six years ago a freshman congressman a young fellow with lots of idealism who was out to change the world stood before sam rayburn in the well of the house and solemnly swore to the same oath that all of you took yesterday an unforgettable experience and i congratulate you all\n",
      "two days later that same freshman stood at the back of this great chamber over there someplace as president truman all charged up by his single handed election victory reported as the constitution requires on the state of the union\n",
      "when the bipartisan applause stopped president truman said i am happy to report to this 81st congress that the state of the union is good our nation is better able than ever before to meet the needs of the american people and to give them their fair chance in the pursuit of happiness it is foremost among the nations of the world in the search for peace\n",
      "today that freshman member from michigan stands where mr truman stood and i must say to you that the state of the union is not good\n",
      "millions of americans are out of work\n"
     ]
    }
   ],
   "source": [
    "with open(\"sotu_1975_2000.txt\") as source:\n",
    "    for i, line in enumerate(source):\n",
    "        print(line.rstrip())\n",
    "        if i >= 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Train a topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task on the topic modelling data is to train an LDA model. For this task you will be using [spaCy](https://spacy.io/) and the [gensim](https://radimrehurek.com/gensim/) topic modelling library.\n",
    "\n",
    "Start by preprocessing the data using spaCy. Given that the data set for this problem is rather small, you do not have to exclude any components from the standard pipeline. Filter out stop words, non-alphabetic tokens, and tokens less than 3 characters in length. Store the documents as a nested list where the first level of nesting corresponds to the sentences and the second level corresponds to the tokens in each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "#function for the preprocess.\n",
    "def preprocess(text):\n",
    "    #convert each row to nlp object\n",
    "    doc = nlp(text)\n",
    "    #We just tokenize, and chech if it is a stop word, is a word and if its size is more than 3\n",
    "    out = [token.text for token in doc if not token.is_stop and token.lemma_.isalpha() and len(token)>=3]\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace the following lines with your own code for preprocessing the documents\n",
    "with open(\"sotu_1975_2000.txt\") as source:\n",
    "    docs = [(preprocess(line)) for line in source]\n",
    "    docs = [' '.join(d) for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your preprocessing by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reduce oil imports million barrels day end year million barrels day end'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'reduce oil imports million barrels day end year million barrels day end'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the list of documents, skim the section [Pre-process and vectorize the documents](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#pre-process-and-vectorize-the-documents) of the gensim documentation to learn how to create the dictionary and the vectorized corpus representation required by gensim. (Note that you cannot use the standard scikit-learn pipeline in this case.) Then, write code to train an [LdaModel](https://radimrehurek.com/gensim/models/ldamodel.html) for $k=10$ topics, and using default values for all other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Despite we have done the preprocessing, we have to do some more steps for the LdaModel\n",
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add bigrams and trigramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose the documents to a dictionary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bug of words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 735\n",
      "Number of documents: 2898\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.035425406, american)</td>\n",
       "      <td>(0.023249242, america)</td>\n",
       "      <td>(0.029464528, year)</td>\n",
       "      <td>(0.038492586, year)</td>\n",
       "      <td>(0.023931947, care)</td>\n",
       "      <td>(0.017707229, child)</td>\n",
       "      <td>(0.024313765, nation)</td>\n",
       "      <td>(0.04530974, child)</td>\n",
       "      <td>(0.031910904, year)</td>\n",
       "      <td>(0.032250308, year)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.023299862, year)</td>\n",
       "      <td>(0.020474348, congress)</td>\n",
       "      <td>(0.021874826, parent)</td>\n",
       "      <td>(0.015445001, new)</td>\n",
       "      <td>(0.021325022, health)</td>\n",
       "      <td>(0.015609585, economy)</td>\n",
       "      <td>(0.024128225, america)</td>\n",
       "      <td>(0.0155935, world)</td>\n",
       "      <td>(0.01549134, way)</td>\n",
       "      <td>(0.014939256, state)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.022288017, america)</td>\n",
       "      <td>(0.017216397, know)</td>\n",
       "      <td>(0.021688573, child)</td>\n",
       "      <td>(0.014699497, security)</td>\n",
       "      <td>(0.020207666, american)</td>\n",
       "      <td>(0.015215455, government)</td>\n",
       "      <td>(0.019209247, people)</td>\n",
       "      <td>(0.0148640415, year)</td>\n",
       "      <td>(0.015356404, budget)</td>\n",
       "      <td>(0.013803592, new)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.020769065, people)</td>\n",
       "      <td>(0.015067735, welfare)</td>\n",
       "      <td>(0.016311307, family)</td>\n",
       "      <td>(0.013771155, tax)</td>\n",
       "      <td>(0.018625494, year)</td>\n",
       "      <td>(0.015017174, job)</td>\n",
       "      <td>(0.017102122, new)</td>\n",
       "      <td>(0.0147505365, community)</td>\n",
       "      <td>(0.014307826, woman)</td>\n",
       "      <td>(0.011687696, million)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.016655918, new)</td>\n",
       "      <td>(0.013275578, community)</td>\n",
       "      <td>(0.015623007, people)</td>\n",
       "      <td>(0.013208683, people)</td>\n",
       "      <td>(0.014187599, new)</td>\n",
       "      <td>(0.012610122, time)</td>\n",
       "      <td>(0.015594441, peace)</td>\n",
       "      <td>(0.010815522, right)</td>\n",
       "      <td>(0.013476645, thank)</td>\n",
       "      <td>(0.011385218, american)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0.016051209, work)</td>\n",
       "      <td>(0.012565485, work)</td>\n",
       "      <td>(0.014388783, american)</td>\n",
       "      <td>(0.013016652, congress)</td>\n",
       "      <td>(0.011710666, congress)</td>\n",
       "      <td>(0.012349059, work)</td>\n",
       "      <td>(0.015561753, world)</td>\n",
       "      <td>(0.010318694, new)</td>\n",
       "      <td>(0.013263569, tax)</td>\n",
       "      <td>(0.010996229, let)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.0118882395, government)</td>\n",
       "      <td>(0.0115534095, people)</td>\n",
       "      <td>(0.012226503, need)</td>\n",
       "      <td>(0.0120803425, million)</td>\n",
       "      <td>(0.011584532, health_care)</td>\n",
       "      <td>(0.011676198, today)</td>\n",
       "      <td>(0.01348151, time)</td>\n",
       "      <td>(0.010006226, crime)</td>\n",
       "      <td>(0.0129640065, cut)</td>\n",
       "      <td>(0.010603567, thank)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.011311045, job)</td>\n",
       "      <td>(0.010618406, year)</td>\n",
       "      <td>(0.011145942, million)</td>\n",
       "      <td>(0.011354099, social)</td>\n",
       "      <td>(0.010527725, security)</td>\n",
       "      <td>(0.010497214, new)</td>\n",
       "      <td>(0.011556006, school)</td>\n",
       "      <td>(0.009563405, parent)</td>\n",
       "      <td>(0.0109167, congress)</td>\n",
       "      <td>(0.00982483, united)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0.010801815, century)</td>\n",
       "      <td>(0.010606974, let)</td>\n",
       "      <td>(0.010635856, government)</td>\n",
       "      <td>(0.0111279255, social_security)</td>\n",
       "      <td>(0.010005497, reform)</td>\n",
       "      <td>(0.009745688, program)</td>\n",
       "      <td>(0.011127741, state)</td>\n",
       "      <td>(0.009301474, school)</td>\n",
       "      <td>(0.010497916, let)</td>\n",
       "      <td>(0.009521989, congress)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.010301305, community)</td>\n",
       "      <td>(0.0099810865, american)</td>\n",
       "      <td>(0.010463617, cut)</td>\n",
       "      <td>(0.011016185, time)</td>\n",
       "      <td>(0.009465486, century)</td>\n",
       "      <td>(0.009689306, energy)</td>\n",
       "      <td>(0.010008111, work)</td>\n",
       "      <td>(0.009196406, country)</td>\n",
       "      <td>(0.010041815, new)</td>\n",
       "      <td>(0.009413649, god_bless)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0                         1  \\\n",
       "0     (0.035425406, american)    (0.023249242, america)   \n",
       "1         (0.023299862, year)   (0.020474348, congress)   \n",
       "2      (0.022288017, america)       (0.017216397, know)   \n",
       "3       (0.020769065, people)    (0.015067735, welfare)   \n",
       "4          (0.016655918, new)  (0.013275578, community)   \n",
       "5         (0.016051209, work)       (0.012565485, work)   \n",
       "6  (0.0118882395, government)    (0.0115534095, people)   \n",
       "7          (0.011311045, job)       (0.010618406, year)   \n",
       "8      (0.010801815, century)        (0.010606974, let)   \n",
       "9    (0.010301305, community)  (0.0099810865, american)   \n",
       "\n",
       "                           2                                3  \\\n",
       "0        (0.029464528, year)              (0.038492586, year)   \n",
       "1      (0.021874826, parent)               (0.015445001, new)   \n",
       "2       (0.021688573, child)          (0.014699497, security)   \n",
       "3      (0.016311307, family)               (0.013771155, tax)   \n",
       "4      (0.015623007, people)            (0.013208683, people)   \n",
       "5    (0.014388783, american)          (0.013016652, congress)   \n",
       "6        (0.012226503, need)          (0.0120803425, million)   \n",
       "7     (0.011145942, million)            (0.011354099, social)   \n",
       "8  (0.010635856, government)  (0.0111279255, social_security)   \n",
       "9         (0.010463617, cut)              (0.011016185, time)   \n",
       "\n",
       "                            4                          5  \\\n",
       "0         (0.023931947, care)       (0.017707229, child)   \n",
       "1       (0.021325022, health)     (0.015609585, economy)   \n",
       "2     (0.020207666, american)  (0.015215455, government)   \n",
       "3         (0.018625494, year)         (0.015017174, job)   \n",
       "4          (0.014187599, new)        (0.012610122, time)   \n",
       "5     (0.011710666, congress)        (0.012349059, work)   \n",
       "6  (0.011584532, health_care)       (0.011676198, today)   \n",
       "7     (0.010527725, security)         (0.010497214, new)   \n",
       "8       (0.010005497, reform)     (0.009745688, program)   \n",
       "9      (0.009465486, century)      (0.009689306, energy)   \n",
       "\n",
       "                        6                          7                      8  \\\n",
       "0   (0.024313765, nation)        (0.04530974, child)    (0.031910904, year)   \n",
       "1  (0.024128225, america)         (0.0155935, world)      (0.01549134, way)   \n",
       "2   (0.019209247, people)       (0.0148640415, year)  (0.015356404, budget)   \n",
       "3      (0.017102122, new)  (0.0147505365, community)   (0.014307826, woman)   \n",
       "4    (0.015594441, peace)       (0.010815522, right)   (0.013476645, thank)   \n",
       "5    (0.015561753, world)         (0.010318694, new)     (0.013263569, tax)   \n",
       "6      (0.01348151, time)       (0.010006226, crime)    (0.0129640065, cut)   \n",
       "7   (0.011556006, school)      (0.009563405, parent)  (0.0109167, congress)   \n",
       "8    (0.011127741, state)      (0.009301474, school)     (0.010497916, let)   \n",
       "9     (0.010008111, work)     (0.009196406, country)     (0.010041815, new)   \n",
       "\n",
       "                          9  \n",
       "0       (0.032250308, year)  \n",
       "1      (0.014939256, state)  \n",
       "2        (0.013803592, new)  \n",
       "3    (0.011687696, million)  \n",
       "4   (0.011385218, american)  \n",
       "5        (0.010996229, let)  \n",
       "6      (0.010603567, thank)  \n",
       "7      (0.00982483, united)  \n",
       "8   (0.009521989, congress)  \n",
       "9  (0.009413649, god_bless)  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "firstN = 10\n",
    "pd.DataFrame({i:[topic for topic in top_topics[i][0][:firstN]] for i in range(num_topics)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a trained model, run the following cell to print the topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the topics. Do they make sense? Can you &lsquo;label&rsquo; each topic with a short description of what it is about? Do the topics contain any unexpected terms? Summarize your discussion in a short text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "There are many words repeating in every topics with big coefficients. Also we can say that we have some overlaping topics. For example first one can be named as \"family\" topic and third one can be too. There are also clear topics if we only consider highest coefficients. For example number 7 is obviously about \"medical care\". For a solution of these repetition words we can play with parameters of filter_exteremes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Monitoring a topic model for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning an LDA model, it is important to make sure that the training algorithm has converged to a stable posterior distribution. One way to do so is to plot, after each training epochs(or &lsquo;pass&rsquo;, in gensim parlance) the log likelihood of the training data under the posterior. Your last task in this lab is to create such a plot and, based on this, to suggest an appropriate number of epochs.\n",
    "\n",
    "To collect information about the posterior likelihood after each pass, we need to enable the logging facilities of gensim. Once this is done, gensim will add various diagnostics to a log file `gensim.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "#logging.basicConfig(filename=\"gensim.log\", format=\"%(asctime)s:%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "logging.basicConfig(filename=\"gensim.log\", filemode='w', format=\"%(asctime)s:%(levelname)s:%(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will parse the generated logfile and return the list of log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_logfile():\n",
    "    matcher = re.compile(\"(-*\\d+\\.\\d+) per-word .* (\\d+\\.\\d+) perplexity\")\n",
    "    likelihoods = []\n",
    "    with open(\"gensim.log\") as source:\n",
    "        for line in source:\n",
    "            match = matcher.search(line)\n",
    "            if match:\n",
    "                likelihoods.append(float(match.group(1)))\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task now is to re-train your LDA model for 50&nbsp;passes, retrieve the list of log likelihoods, and create a plot from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    passes=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLike = parse_logfile()[-50:] #every time we run the procedure, the result is added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "x = [i for i in range(1,51)]\n",
    "\n",
    "plt.plot(x,logLike, color = \"red\" ,marker='o', linestyle='dashed',linewidth = 1,markersize = 5,alpha = 0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = max(logLike)\n",
    "max_index = [i for i, j in enumerate(logLike) if j == m]\n",
    "\n",
    "max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-6.059, -6.058], -6.083, -6.074)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logLike[48:50], logLike[15],logLike[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret your plot? What would be a reasonable choice for the number of passes? Retrain your LDA model with that number and re-inspect the topics it finds. Do you consider the new topics to be &lsquo;better&rsquo; than the ones that you got from the 1-pass model in Problem&nbsp;5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, it is clear that after a certain number of passes( more than 10), the likelihood does not change signifancly. Therefore a value close to 15 should be the optimal one. By the term optimal, we mean that provides a high log likelihood value, but also does not make the model more complicated or computational expensive. For example, the maximum value(as printed above) is for 48,49 passes but that numbers make the model more expensive. Therefore, we can choose 15 as the optimal number of passes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Optimal LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_passes = 15\n",
    "Opt_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    passes=opt_passes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.07616353, child)</td>\n",
       "      <td>(0.03819304, america)</td>\n",
       "      <td>(0.024330353, policy)</td>\n",
       "      <td>(0.041645534, new)</td>\n",
       "      <td>(0.03162559, congress)</td>\n",
       "      <td>(0.04599714, people)</td>\n",
       "      <td>(0.053688128, year)</td>\n",
       "      <td>(0.058887314, school)</td>\n",
       "      <td>(0.09828385, health)</td>\n",
       "      <td>(0.034869365, woman)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.03717549, family)</td>\n",
       "      <td>(0.035835367, world)</td>\n",
       "      <td>(0.023880599, energy)</td>\n",
       "      <td>(0.040873628, america)</td>\n",
       "      <td>(0.030943846, federal)</td>\n",
       "      <td>(0.03430715, american)</td>\n",
       "      <td>(0.039287157, budget)</td>\n",
       "      <td>(0.037220232, community)</td>\n",
       "      <td>(0.059104513, care)</td>\n",
       "      <td>(0.02846766, let)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.028260766, work)</td>\n",
       "      <td>(0.030244047, year)</td>\n",
       "      <td>(0.019464584, soviet)</td>\n",
       "      <td>(0.02679306, peace)</td>\n",
       "      <td>(0.02999214, government)</td>\n",
       "      <td>(0.026949314, work)</td>\n",
       "      <td>(0.025542364, cut)</td>\n",
       "      <td>(0.028644903, new)</td>\n",
       "      <td>(0.050641775, american)</td>\n",
       "      <td>(0.027430907, thank)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.025314739, parent)</td>\n",
       "      <td>(0.025998933, american)</td>\n",
       "      <td>(0.017608682, nation)</td>\n",
       "      <td>(0.025792995, world)</td>\n",
       "      <td>(0.02709165, program)</td>\n",
       "      <td>(0.020315956, year)</td>\n",
       "      <td>(0.023851575, tax)</td>\n",
       "      <td>(0.026468458, education)</td>\n",
       "      <td>(0.04316469, president)</td>\n",
       "      <td>(0.023037525, family)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.023151498, job)</td>\n",
       "      <td>(0.024303798, nation)</td>\n",
       "      <td>(0.013469404, force)</td>\n",
       "      <td>(0.025002385, government)</td>\n",
       "      <td>(0.023408428, social)</td>\n",
       "      <td>(0.018766822, let)</td>\n",
       "      <td>(0.021784887, rate)</td>\n",
       "      <td>(0.019400798, year)</td>\n",
       "      <td>(0.041692816, health_care)</td>\n",
       "      <td>(0.020878782, credit)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0.022226708, know)</td>\n",
       "      <td>(0.021659255, people)</td>\n",
       "      <td>(0.012951509, program)</td>\n",
       "      <td>(0.02167129, economy)</td>\n",
       "      <td>(0.020763319, reform)</td>\n",
       "      <td>(0.018147927, way)</td>\n",
       "      <td>(0.01850364, deficit)</td>\n",
       "      <td>(0.018463697, help)</td>\n",
       "      <td>(0.03520802, fellow)</td>\n",
       "      <td>(0.01975919, tax_credit)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.021293236, tax)</td>\n",
       "      <td>(0.017860785, tonight)</td>\n",
       "      <td>(0.012312631, national)</td>\n",
       "      <td>(0.018071348, century)</td>\n",
       "      <td>(0.018204425, year)</td>\n",
       "      <td>(0.017314201, crime)</td>\n",
       "      <td>(0.018015588, growth)</td>\n",
       "      <td>(0.018237062, teacher)</td>\n",
       "      <td>(0.034602337, member)</td>\n",
       "      <td>(0.0190401, year)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.019682452, million)</td>\n",
       "      <td>(0.017379686, state)</td>\n",
       "      <td>(0.012099611, major)</td>\n",
       "      <td>(0.016702743, nation)</td>\n",
       "      <td>(0.01719419, security)</td>\n",
       "      <td>(0.013369702, drug)</td>\n",
       "      <td>(0.0171856, economic)</td>\n",
       "      <td>(0.016497415, national)</td>\n",
       "      <td>(0.03210087, insurance)</td>\n",
       "      <td>(0.017215528, term)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0.018232374, people)</td>\n",
       "      <td>(0.014828597, country)</td>\n",
       "      <td>(0.011640484, administration)</td>\n",
       "      <td>(0.014563746, community)</td>\n",
       "      <td>(0.016697904, social_security)</td>\n",
       "      <td>(0.012857484, time)</td>\n",
       "      <td>(0.016605586, percent)</td>\n",
       "      <td>(0.015411655, high)</td>\n",
       "      <td>(0.029428132, medicare)</td>\n",
       "      <td>(0.0155392, great)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.017439561, right)</td>\n",
       "      <td>(0.013213176, freedom)</td>\n",
       "      <td>(0.011535951, new)</td>\n",
       "      <td>(0.014294162, responsibility)</td>\n",
       "      <td>(0.01589161, legislation)</td>\n",
       "      <td>(0.012758722, college)</td>\n",
       "      <td>(0.016369749, spending)</td>\n",
       "      <td>(0.014073875, student)</td>\n",
       "      <td>(0.0229043, vice)</td>\n",
       "      <td>(0.015483962, american)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0                        1  \\\n",
       "0     (0.07616353, child)    (0.03819304, america)   \n",
       "1    (0.03717549, family)     (0.035835367, world)   \n",
       "2     (0.028260766, work)      (0.030244047, year)   \n",
       "3   (0.025314739, parent)  (0.025998933, american)   \n",
       "4      (0.023151498, job)    (0.024303798, nation)   \n",
       "5     (0.022226708, know)    (0.021659255, people)   \n",
       "6      (0.021293236, tax)   (0.017860785, tonight)   \n",
       "7  (0.019682452, million)     (0.017379686, state)   \n",
       "8   (0.018232374, people)   (0.014828597, country)   \n",
       "9    (0.017439561, right)   (0.013213176, freedom)   \n",
       "\n",
       "                               2                              3  \\\n",
       "0          (0.024330353, policy)             (0.041645534, new)   \n",
       "1          (0.023880599, energy)         (0.040873628, america)   \n",
       "2          (0.019464584, soviet)            (0.02679306, peace)   \n",
       "3          (0.017608682, nation)           (0.025792995, world)   \n",
       "4           (0.013469404, force)      (0.025002385, government)   \n",
       "5         (0.012951509, program)          (0.02167129, economy)   \n",
       "6        (0.012312631, national)         (0.018071348, century)   \n",
       "7           (0.012099611, major)          (0.016702743, nation)   \n",
       "8  (0.011640484, administration)       (0.014563746, community)   \n",
       "9             (0.011535951, new)  (0.014294162, responsibility)   \n",
       "\n",
       "                                4                       5  \\\n",
       "0          (0.03162559, congress)    (0.04599714, people)   \n",
       "1          (0.030943846, federal)  (0.03430715, american)   \n",
       "2        (0.02999214, government)     (0.026949314, work)   \n",
       "3           (0.02709165, program)     (0.020315956, year)   \n",
       "4           (0.023408428, social)      (0.018766822, let)   \n",
       "5           (0.020763319, reform)      (0.018147927, way)   \n",
       "6             (0.018204425, year)    (0.017314201, crime)   \n",
       "7          (0.01719419, security)     (0.013369702, drug)   \n",
       "8  (0.016697904, social_security)     (0.012857484, time)   \n",
       "9       (0.01589161, legislation)  (0.012758722, college)   \n",
       "\n",
       "                         6                         7  \\\n",
       "0      (0.053688128, year)     (0.058887314, school)   \n",
       "1    (0.039287157, budget)  (0.037220232, community)   \n",
       "2       (0.025542364, cut)        (0.028644903, new)   \n",
       "3       (0.023851575, tax)  (0.026468458, education)   \n",
       "4      (0.021784887, rate)       (0.019400798, year)   \n",
       "5    (0.01850364, deficit)       (0.018463697, help)   \n",
       "6    (0.018015588, growth)    (0.018237062, teacher)   \n",
       "7    (0.0171856, economic)   (0.016497415, national)   \n",
       "8   (0.016605586, percent)       (0.015411655, high)   \n",
       "9  (0.016369749, spending)    (0.014073875, student)   \n",
       "\n",
       "                            8                         9  \n",
       "0        (0.09828385, health)      (0.034869365, woman)  \n",
       "1         (0.059104513, care)         (0.02846766, let)  \n",
       "2     (0.050641775, american)      (0.027430907, thank)  \n",
       "3     (0.04316469, president)     (0.023037525, family)  \n",
       "4  (0.041692816, health_care)     (0.020878782, credit)  \n",
       "5        (0.03520802, fellow)  (0.01975919, tax_credit)  \n",
       "6       (0.034602337, member)         (0.0190401, year)  \n",
       "7     (0.03210087, insurance)       (0.017215528, term)  \n",
       "8     (0.029428132, medicare)        (0.0155392, great)  \n",
       "9           (0.0229043, vice)   (0.015483962, american)  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics = Opt_model.top_topics(corpus)\n",
    "firstN = 10\n",
    "pd.DataFrame({i:[topic for topic in top_topics[i][0][:firstN]] for i in range(num_topics)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Coefficients are much more accurate than previous results, but we stil have some repetitions even for high coefficients. Some of them are more clear, for example 7:education, 8:healt care, 6:budget/economy, 4:government, 0:child/parenting and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Please read the section â€˜General informationâ€™ on the â€˜Labsâ€™ page of the course website before submitting this notebook!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
